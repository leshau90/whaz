{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c districtdatalabs yellowbrick\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from ydata_profiling import ProfileReport\n",
    "import sklearn\n",
    "# sklearn.set_config(transform_output=\"pandas\")\n",
    "# import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labeller  =  preprocessing.LabelEncoder()\n",
    "ohe = preprocessing.OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items = pd.read_csv('./dataset/olist_order_items_dataset.csv')\n",
    "customers = pd.read_csv('./dataset/olist_customers_dataset.csv')\n",
    "sellers = pd.read_csv('./dataset/olist_sellers_dataset.csv')\n",
    "products = pd.read_csv('./dataset/olist_products_dataset.csv')\n",
    "orders = pd.read_csv('./dataset/olist_orders_dataset.csv')\n",
    "reviews = pd.read_csv('./dataset/olist_order_reviews_dataset.csv')\n",
    "geos = pd.read_csv('./dataset/olist_geolocation_dataset.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000163 entries, 0 to 1000162\n",
      "Data columns (total 5 columns):\n",
      " #   Column                       Non-Null Count    Dtype  \n",
      "---  ------                       --------------    -----  \n",
      " 0   geolocation_zip_code_prefix  1000163 non-null  int64  \n",
      " 1   geolocation_lat              1000163 non-null  float64\n",
      " 2   geolocation_lng              1000163 non-null  float64\n",
      " 3   geolocation_city             1000163 non-null  object \n",
      " 4   geolocation_state            1000163 non-null  object \n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 38.2+ MB\n"
     ]
    }
   ],
   "source": [
    "geos.info()\n",
    "# geos.groupby(by=['geolocation_zip_code_prefix']).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "orders[['order_purchase_timestamp',\n",
    "        'order_approved_at',\n",
    "        'order_delivered_carrier_date',\n",
    "        'order_delivered_customer_date',\n",
    "        'order_estimated_delivery_date']] = orders[['order_purchase_timestamp',\n",
    "                                                    'order_approved_at',\n",
    "                                                    'order_delivered_carrier_date',\n",
    "                                                    'order_delivered_customer_date',\n",
    "                                                    'order_estimated_delivery_date']].apply(pd.to_datetime) \n",
    "reviews[['review_creation_date',\n",
    "        'review_answer_timestamp']] = reviews[['review_creation_date',\n",
    "                                                    'review_answer_timestamp']].apply(pd.to_datetime) \n",
    "order_items[['shipping_limit_date']] = order_items[['shipping_limit_date']].apply(pd.to_datetime)\n",
    "# order_items.info(),orders.info()\n",
    "# order_items.head()\n",
    "# customers.head(),order_items.head(),orders.head(),customers[customers['customer_id'] == \"9ef432eb6251297304e76186b10a928d\"], customers[customers['customer_unique_id'] == \"9ef432eb6251297304e76186b10a928d\"]\n",
    "# ProfileReport(order_items,title=\"order of items\").to_widgets()\n",
    "\n",
    "\n",
    "merged = pd.merge(orders,order_items)\n",
    "merged = merged.merge(customers)\n",
    "merged = merged.merge(sellers)\n",
    "merged = merged.merge(products)\n",
    "merged = merged.merge(reviews)\n",
    "\n",
    "\n",
    "merged['product_photos_qty']=merged['product_photos_qty'].fillna(0)\n",
    "# merged[merged['product_photos_qty'].isna()][['customer_id','product_photos_qty']]\n",
    "\n",
    "# merged[['seller_city','seller_state','customer_city','customer_state']] = merged[['seller_city','seller_state','customer_city','customer_state']].astype('category') \n",
    "# merged[['seller_city','seller_state','customer_city','customer_state']].info()\n",
    "# print(merged[['seller_city','seller_state','customer_city','customer_state','review_score']].head())\n",
    "# print(merged['customer_city'].value_counts())\n",
    "# merged['seller_city'] = labeller.fit_transform(merged['seller_city'].astype('str'))\n",
    "# merged['seller_state'] = labeller.fit_transform(merged['seller_state'].astype('str'))\n",
    "# merged['customer_city'] = labeller.fit_transform(merged['customer_city'].astype('str'))\n",
    "# merged['customer_state'] = labeller.fit_transform(merged['customer_state'].astype('str'))\n",
    "# merged['review_score'] = labeller.fit_transform(merged['review_score'].astype('str'))\n",
    "\n",
    "# print(merged[['seller_city','seller_state','customer_city','customer_state','review_score']].head())\n",
    "# merged[['seller_city','seller_state','customer_city','customer_state','review_score']].info()\n",
    "\n",
    "\n",
    "merged['time_to_get'] = merged.order_delivered_customer_date - merged['order_purchase_timestamp']\n",
    "merged['time_to_get'] = merged['time_to_get'].dt.seconds/60\n",
    "merged['time_to_get'] = merged['time_to_get']\n",
    "\n",
    "\n",
    "# merged['time_to_get'] = merged['time_to_get'].replace(np.nan, 0)\n",
    "# merged.time_to_get.isna().sum()\n",
    "\n",
    "# merged.info()\n",
    "# ProfileReport(merged,title=\"order of items\").to_widgets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.merge(geos[['geolocation_zip_code_prefix','geolocation_lat','geolocation_lng']],\n",
    "                      left_on=['seller_zip_code_prefix'],\n",
    "                      right_on=['geolocation_zip_code_prefix'],\n",
    "                      how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tohe=ohe.fit_transform(merged[['seller_city']])\n",
    "# Perform one-hot encoding\n",
    "# one_hot = pd.get_dummies(merged['seller_city'], prefix='seller_from')\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original DataFrame\n",
    "# df_encoded = pd.concat([merged, one_hot], axis=1)\n",
    "# one_hot.head()\n",
    "# df_encoded[['order_id','seller_city','seller_from_maua']].head()\n",
    "# df_encoded.filter(like='ma')\n",
    "# Remove the original 'city' column if desired\n",
    "# df_encoded.drop('city', axis=1, inplace=True)\n",
    "\n",
    "# Print the encoded DataFrame\n",
    "# print(df_encoded)\n",
    "# merged.groupby(['seller_state','customer_state']).size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some dropper\n",
    "oriC =np.array( merged.columns)\n",
    "# ['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp',\n",
    "#        'order_approved_at', 'order_delivered_carrier_date',\n",
    "#        'order_delivered_customer_date', 'order_estimated_delivery_date',\n",
    "#        'order_item_id', 'product_id', 'seller_id', 'shipping_limit_date',\n",
    "#        'price', 'freight_value', 'customer_unique_id',\n",
    "#        'customer_zip_code_prefix', 'customer_city', 'customer_state',\n",
    "#        'seller_zip_code_prefix', 'seller_city', 'seller_state',\n",
    "#        'product_category_name', 'product_name_lenght',\n",
    "#        'product_description_lenght', 'product_photos_qty', 'product_weight_g',\n",
    "#        'product_length_cm', 'product_height_cm', 'product_width_cm',\n",
    "#        'review_id', 'review_score', 'review_comment_title',\n",
    "#        'review_comment_message', 'review_creation_date',\n",
    "#        'review_answer_timestamp', 'time_to_get']\n",
    "onlyThis = np.delete(oriC,np.where(np.isin(oriC,[\n",
    "       # 'order_id','review_score', 'customer_id',  'time_to_get', 'price', 'freight_value',\n",
    "       'order_status', \n",
    "       'order_purchase_timestamp',\n",
    "       'order_approved_at', 'order_delivered_carrier_date',\n",
    "       'order_delivered_customer_date', 'order_estimated_delivery_date',\n",
    "       'order_item_id', 'product_id', \n",
    "       'seller_id', 'shipping_limit_date','customer_unique_id',\n",
    "       'customer_zip_code_prefix', \n",
    "       'customer_city', 'customer_state',\n",
    "       'seller_zip_code_prefix', \n",
    "       'seller_city', 'seller_state',\n",
    "       'product_category_name', 'product_name_lenght',\n",
    "       'product_description_lenght', \n",
    "       'product_photos_qty', 'product_weight_g',\n",
    "       'product_length_cm', 'product_height_cm', 'product_width_cm',\n",
    "       'review_id', 'review_score', 'review_comment_title',\n",
    "       'review_comment_message', 'review_creation_date',\n",
    "       'review_answer_timestamp'])))\n",
    "\n",
    "# onlyThis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data and define features and target variable\n",
    "# merged = pd.read_csv('order_data.csv')\n",
    "X = merged[['price','freight_value','seller_city','seller_state'\n",
    "            ,'customer_city','customer_state','time_to_get']]\n",
    "# merged['review_score_transformed'] = merged['review_score']-1\n",
    "y = merged['review_score']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the XGBoost model\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# # Train the SVM model\n",
    "# svm_model = SVC()\n",
    "# svm_model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "# svm_pred = svm_model.predict(X_test)\n",
    "\n",
    "# # Classification report for XGBoost model\n",
    "# xgb_report = classification_report(y_test, xgb_pred)\n",
    "# print(\"XGBoost Classification Report:\")\n",
    "# print(xgb_report)\n",
    "\n",
    "# # # Classification report for Random Forest model\n",
    "# rf_report = classification_report(y_test, rf_pred)\n",
    "# print(\"Random Forest Classification Report:\")\n",
    "# print(rf_report)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Classification report for SVM model\n",
    "# svm_report = classification_report(y_test, svm_pred)\n",
    "# print(\"SVM Classification Report:\")\n",
    "# print(svm_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Random Logistic Regression\n",
    "logreg_model = LogisticRegression(multi_class='multinomial', solver='lbfgs',max_iter=200000)\n",
    "logreg_model.fit(X_train,y_train)\n",
    "\n",
    "logr_pred = logreg_model.predict(X_test)\n",
    "\n",
    "# logr_report = classification_report(y_test, logr_pred,zero_division=0)\n",
    "# print(\"Logistic Regression Classification Report:\")\n",
    "# print(logr_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reports = [logr_report,xgb_report,rf_report]\n",
    "# reports_name = ['mlr','xgb','rforest']\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# # from sklearn.metrics import classification_report\n",
    "# accuracy_list = []\n",
    "# model_labels = []\n",
    "\n",
    "# # Iterate over the classification reports\n",
    "# for i, report in enumerate(reports):\n",
    "#     # Split the report lines and extract the accuracy value\n",
    "#     # print(lines)\n",
    "#     # print(report)\n",
    "#     lines = report.split('\\n')\n",
    "#     accuracy_line = lines[8]\n",
    "#     accuracy = float(accuracy_line.split()[-2])\n",
    "\n",
    "#     # Add accuracy to the list\n",
    "#     accuracy_list.append(accuracy)\n",
    "#     model_labels.append(f\"{reports_name[i]} Accuracy\")\n",
    "\n",
    "# # Plot the accuracy values\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# x = range(len(model_labels))\n",
    "# plt.bar(x, accuracy_list)\n",
    "# plt.xticks(x, model_labels)\n",
    "# # plt.xlabel('Models')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Accuracy of Classification Models')\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scikitplot as skplt\n",
    "\n",
    "\n",
    "# skplt.metrics.plot_confusion_matrix(y_test, xgb_pred, normalize=False, title = 'xgb')\n",
    "# skplt.metrics.plot_confusion_matrix(y_test, rf_pred, normalize=False, title = 'rf' )\n",
    "# skplt.metrics.plot_confusion_matrix(y_test, logr_pred, normalize=False, title = 'logreg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay.from_predictions(\n",
    "                                              y_test, \n",
    "                                              xgb_pred, \n",
    "                                            #   display_labels=labels, \n",
    "                                              cmap=plt.cm.Blues\n",
    "                                              ) \n",
    "fig = disp.figure_\n",
    "fig.set_figwidth(5)\n",
    "fig.set_figheight(5) \n",
    "fig.suptitle('XGB')\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_predictions(\n",
    "                                              y_test, \n",
    "                                              rf_pred, \n",
    "                                            #   display_labels=labels, \n",
    "                                              cmap=plt.cm.Blues\n",
    "                                              ) \n",
    "fig = disp.figure_\n",
    "fig.set_figwidth(5)\n",
    "fig.set_figheight(5) \n",
    "fig.suptitle('RF')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightgbm import LGBMClassifier\n",
    "\n",
    "# LGBMC = LGBMClassifier()\n",
    "# merged.review_score.value_counts().plot(kind='bar')\n",
    "\n",
    "# merged.pivot_table(index='review_score',aggfunc='count')\n",
    "merged.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Data Resampling: Data resampling techniques involve either oversampling the minority class or undersampling the majority class to achieve a balanced distribution.\n",
    "    * Oversampling: It involves creating synthetic samples for the minority class, such as using techniques like SMOTE, ADASYN, or random oversampling.\n",
    "    * Undersampling: It involves randomly removing samples from the majority class, reducing its dominance. It can be achieved using techniques like RandomUnderSampler, Tomek links, or Cluster Centroids.\n",
    "    * Class Weighting: Many machine learning algorithms allow assigning higher weights to minority class samples during model training. This gives more importance to the minority class and helps to mitigate the impact of class imbalance. Class weights can be manually specified or automatically computed based on the class frequencies.\n",
    "    * Algorithmic Approaches: Certain algorithms are inherently robust to class imbalance or have built-in techniques to handle it. For example:\n",
    "    * Decision Trees: Decision trees can naturally handle imbalanced datasets by splitting nodes based on the class distribution.\n",
    "    * Ensemble Methods: Ensemble algorithms like Random Forest and Gradient Boosting are often effective in dealing with class imbalance due to their aggregation of multiple models.\n",
    "    * Cost-Sensitive Learning: Cost-sensitive learning involves assigning misclassification costs to different classes. It aims to minimize the cost of misclassifying the minority class by penalizing such errors more heavily during model training.\n",
    "    * Collect More Data: Increasing the amount of data, particularly for the minority class, can help address imbalance. Obtaining more samples can improve the representation of the minority class, making the model more robust.\n",
    "\n",
    "--------\n",
    "    Determining the number of new rows to generate for the minority class in an imbalanced dataset is not a straightforward task and depends on several factors. There is no fixed rule or a universally applicable threshold for the number of new rows that would be considered a \"healthy addition.\" The appropriate number of new rows can vary depending on the specific problem, the available data, and the degree of class imbalance.\n",
    "\n",
    "Here are a few considerations to help guide the decision-making process:\n",
    "\n",
    "    Proportional Representation: Aim to achieve a more balanced distribution by increasing the representation of the minority class. The goal is to have a proportionate number of samples across classes. However, it's important to strike a balance and not over-represent the minority class to the extent that it dominates the dataset.\n",
    "\n",
    "    Impact on Performance: Consider the impact of the generated samples on the overall model performance. Generating too few samples may not sufficiently address the imbalance, while generating too many samples can introduce noise or overfitting. It's advisable to monitor the performance of the model on a validation set or through cross-validation to find the optimal number of new rows.\n",
    "\n",
    "    Domain Knowledge: Take into account the characteristics of the specific problem domain. Some domains may inherently have a high degree of class imbalance, while others may require a more balanced representation. Consulting with domain experts or understanding the consequences of misclassifications can help guide the decision.\n",
    "\n",
    "    Available Data: Consider the size and quality of the original dataset. If the original dataset is small, generating a large number of new rows for the minority class might lead to an over-reliance on synthetic samples. Conversely, if the original dataset is large, generating a moderate number of new rows may be sufficient to address the imbalance.\n",
    "\n",
    "    Evaluation Metrics: Assess the model's performance using appropriate evaluation metrics, such as precision, recall, F1 score, or area under the ROC curve (AUC-ROC). Iteratively adjust the number of generated samples until the desired performance is achieved.\n",
    "\n",
    "It is recommended to experiment with different numbers of generated samples and monitor the model's performance on validation data to find the optimal balance between addressing class imbalance and preserving the generalization ability of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
